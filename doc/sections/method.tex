\documentclass[report.tex]{subfile}

\begin{document}

\section{Method}
A classifier has been created by training a multilayer perceptron neural
network with the given training data of hand written digits. The given data is
first partitioned into three subsets. The training data is preprocessed and
turned into rasterized images of the digits. The images are then used to train
the neural network.

\subsection{Partitioning of training data}
The training data is split up into three different subsets. Two sets are used
for the training process; the training set and the validation set. The training
set is used to train multiple classifiers with different parameters, and the
validation set is used to select which of these classifiers to use. The third
and final set, the test set, is used after the training is completed in order
to evaluate how well the classifier performs. The subsets were selected
randomly but uniformly for each digit. The distribution was 70\%, 10\% and 20\%
for the training, validation and test set respectively.

\subsection{Preprocessing}
The input data for each data is given as a list of 3D-coordinates that together
form a curve in the shape of a digit. As the digits are aligned along the plane
of the XY axes, the Z-coordinate is simply ignored and the curves are treated
as 2D only. The digits are first translated and scaled to a common location and
size and then rasterized into a grayscale image. The grayscale image is then
dilated so that the digits become thicker. The intensity of each pixel is
finally treated as a large vector. Each pixel represents an input neuron in the
neural network.

\subsubsection{Scaling}
\begin{figure}
    \centering
    \begin{subfigure}[c]{0.3\textwidth}
        \resizebox{\textwidth}{!}{\input{build/fig/scale_3d1.tex}}
        \caption{Six \#1, raw}
    \end{subfigure}%
    {\large $\rightarrow$}
    \begin{subfigure}[c]{0.3\textwidth}
        \resizebox{\textwidth}{!}{\input{build/fig/scale_before1.tex}}
        \caption{Six \#1, projected}
    \end{subfigure}%
    {\large $\rightarrow$}
    \begin{subfigure}[c]{0.3\textwidth}
        \resizebox{\textwidth}{!}{\input{build/fig/scale_after1.tex}}
        \caption{Six \#1, scaled}
    \end{subfigure}\\
    \begin{subfigure}[c]{0.3\textwidth}
        \resizebox{\textwidth}{!}{\input{build/fig/scale_3d2.tex}}
        \caption{Six \#1, projected}
    \end{subfigure}%
    {\large $\rightarrow$}
    \begin{subfigure}[c]{0.3\textwidth}
        \resizebox{\textwidth}{!}{\input{build/fig/scale_before2.tex}}
        \caption{Six \#2, raw}
    \end{subfigure}%
    {\large $\rightarrow$}
    \begin{subfigure}[c]{0.3\textwidth}
        \resizebox{\textwidth}{!}{\input{build/fig/scale_after2.tex}}
        \caption{Six \#2, scaled}
    \end{subfigure}
    \caption{Two sixes before and after projection and scaling.}
    \label{fig:scaling}
\end{figure}
Because the digits are written at different locations and in different sizes,
scaling and translation is performed to make similar shaped images comparable.
Only the shape of the digit is useful, the location and scale can be changed
without losing any useful information. Figure \ref{fig:scaling} shows two sixes
being scaled.

\subsubsection{Rasterization}
\begin{figure}
    \centering
    \begin{subfigure}[c]{0.3\textwidth}
        \vspace*{-3.5mm}
        \resizebox{\textwidth}{!}{\input{build/fig/rasterize_0.tex}}
        \vspace*{-10mm}
        \caption{Scaled}
    \end{subfigure}%
    {\large $\rightarrow$}
    \begin{subfigure}[c]{0.3\textwidth}
        \centering
        \resizebox{0.8\textwidth}{!}{%
            \includegraphics{build/fig/rasterize_1.png}%
        }
        \vfill
        \caption{Rasterized}
    \end{subfigure}%
    {\large $\rightarrow$}
    \begin{subfigure}[c]{0.3\textwidth}
        \centering
        \resizebox{0.8\textwidth}{!}{%
            \includegraphics{build/fig/rasterize_2.png}%
        }
        \vfill
        \caption{Dilated}
    \end{subfigure}%
    \caption{Rasterization of a two with some noise.}
    \label{fig:raster_noise}
\end{figure}
When the digits are located in the same position and are of similar scale the
digits can be rasterized. The rasterization is performed by rasterizing each
line of the curve. The rasterization is performed by using Xiaolin Wu's line
drawing algorithm. The algorithm is described in Wu's article in \emph{Computer
Graphics}\cite{wu-line}. The rasterization is done such that the image is $W
\times W$ pixels large with $W = 20$.

Each rasterized curve is then added to the final image by taking the maximum
intensity of the previous pixel and the pixel of the curve.

Finally, dilation is performed to make the digits thicker. The dilation is
performed by applying a two-dimensional convolution to the image with the
matrix
\begin{equation*}
    D =
    \begin{bmatrix}
        0.5 & 0.8 & 0.5 \\
        0.8 &   1 & 0.8 \\
        0.5 & 0.8 & 0.5
    \end{bmatrix}.
\end{equation*}
In this way, the inner pixels of digits will be 1 and the edges will be between
0 and 1, which could be useful for the classifier because the same digit
usually have edges in around the same location.

One good thing about rasterization is that it decreases the impact of noise as
is demonstrated in figure \ref{fig:raster_noise}. Also, duplicate overlapping
lines will look the same as if there was only one line.

\subsection{Neural network - multilayer perceptron}
\begin{figure}
    \centering
    \input{doc/fig/mlp.tex}
    \caption{}
    \label{fig:mlp}
\end{figure}
The classifier was implemented using a multi-layer perceptron with one hidden
layer. The input layer consists of all the intensities of the pixels of the
grayscale image resulting from the preprocessing of the input. An additional
fully white pixel is added in order to provide a bias for the weights.

The details of a multi-layer perceptron is described in chapter 6 of the book
\emph{Pattern Classification}\cite{hart-pattern}. Here will only be described
shortly how the classifier of this project was implemented.

\subsubsection{Feedforward}
W -- input \\
H -- hidden \\
D -- output \\

\begin{align}
&x \in M_{W \times 1}, \qquad
\overline x = \left( x, 1 \right) \in M_{(W+1) \times 1}, \\
&y^h \in M_{H \times 1}, \qquad
\overline y^h = \left( y^h, 1 \right) \in M_{(H+1) \times 1}, \\
&y^o \in M_{D \times 1}, \\
&w^h \in M_{H \times (W + 1)}, \qquad
\overline w^h \in M_{H \times W}, \\
&w^o \in M_{D \times (H + 1)}, \qquad
\overline w^o \in M_{D \times H} \\
&y^h_i = \tanh \left( \sum_{k=1}^{W} x_k w^h_{i,k} + w^h_{i,W+1} \right) = \tanh \left( w^h_i \cdot \overline x \right), \\
&i = 1..H, \\
&y^o_j = \sum_{i=1}^{H} y^h_i w^o_{j,i} + w^o_{j,H+1} = w^o_j \cdot \overline y^h, \\
&j = 1..D, \\
&J = \frac{(y^o - y^w)^2}{2}, \\
&w^h_{i,k} = w^h_{i,k} - \rho \; \frac{\partial J}{\partial w^h_{i,k}} \\
&w^o_{j,i} = w^o_{j,i} - \rho \; \frac{\partial J}{\partial w^o_{j,i}}
\end{align}

\subsubsection{Backpropagation}
% TODO math expressions

\subsubsection{Training}
A classifier, i.e. the weights for the network, are created by applying the
feedforward and backpropagation operation on all of the preprocessed digits.
This is done in multiple iterations each time using the previous weights for
the current iteration. The initial weights are selected randomly. The
implementation performs up to 1000 iterations before abrupting.

This whole process is done multiple times but with different parameters. In the
implementation, the parameter that is altered between the iterations is the
number of neurons in the hidden layer. This implemation uses 16 different
numbers of hidden neurons, ranging from 10 to 25. 

When multiple classifiers has been created, the one that performs the best on
the validation set is selected and used. The performance is measured by the
percentage of correctly classified digits, i.e. the average of the f1 score for
all the classes.

\subsubsection{Classification}
In order to classify a digit the the digit is preprocessed as described above
and the resulting pixels are used as the input for the neural network with the
weights provided by the training. The feedforward operation is performed and
the strongest output neuron determines the classification of the digit.

\end{document}
